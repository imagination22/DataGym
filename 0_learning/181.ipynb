{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d17d4d40-66e2-4df1-8f20-f1f9ca8f9c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ§  Leetcode 181 â€” Employees Earning More Than Their Managers (Databricks Edition)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Problem Statement\n",
    "\n",
    "### Table: Employee\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| salary      | int     |\n",
    "| managerId   | int     |\n",
    "\n",
    "- `id` is the primary key.\n",
    "- Each row indicates the ID of an employee, their name, salary, and the ID of their manager.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "Write a query to find the employees who earn **more than their managers**.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Example\n",
    "\n",
    "### Input\n",
    "\n",
    "**Employee Table**\n",
    "\n",
    "| id | name  | salary | managerId |\n",
    "|----|-------|--------|-----------|\n",
    "| 1  | Joe   | 70000  | 3         |\n",
    "| 2  | Henry | 80000  | 4         |\n",
    "| 3  | Sam   | 60000  | Null      |\n",
    "| 4  | Max   | 90000  | Null      |\n",
    "\n",
    "### Output\n",
    "\n",
    "| Employee |\n",
    "|----------|\n",
    "| Joe      |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Joe earns 70000, while his manager (Sam) earns 60000.  \n",
    "Henry earns 80000, but his manager (Max) earns 90000 â€” so heâ€™s excluded.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± PySpark DataFrame Creation\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "employee_data = [\n",
    "    Row(id=1, name=\"Joe\", salary=70000, managerId=3),\n",
    "    Row(id=2, name=\"Henry\", salary=80000, managerId=4),\n",
    "    Row(id=3, name=\"Sam\", salary=60000, managerId=None),\n",
    "    Row(id=4, name=\"Max\", salary=90000, managerId=None)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data)\n",
    "\n",
    "# Register temp view\n",
    "employee_df.createOrReplaceTempView(\"Employee\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… SQL Solution\n",
    "\n",
    "```sql\n",
    "SELECT e.name AS Employee\n",
    "FROM Employee e\n",
    "JOIN Employee m\n",
    "ON e.managerId = m.id\n",
    "WHERE e.salary > m.salary;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª PySpark Solution\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "emp = employee_df.alias(\"e\")\n",
    "mgr = employee_df.alias(\"m\")\n",
    "\n",
    "result_df = emp.join(\n",
    "    mgr,\n",
    "    emp[\"managerId\"] == mgr[\"id\"],\n",
    "    how=\"inner\"\n",
    ").filter(\n",
    "    emp[\"salary\"] > mgr[\"salary\"]\n",
    ").select(\n",
    "    emp[\"name\"].alias(\"Employee\")\n",
    ")\n",
    "\n",
    "result_df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ *This notebook is part of DataGymâ€™s SQL-to-PySpark transition series. Want to build a reusable template for join-based logic problems? Letâ€™s co-create it!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "633bd1d9-6734-4a8d-bb20-6ed0f1064147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "employee_data = [\n",
    "    Row(id=1, name=\"Joe\", salary=70000, managerId=3),\n",
    "    Row(id=2, name=\"Henry\", salary=80000, managerId=4),\n",
    "    Row(id=3, name=\"Sam\", salary=60000, managerId=None),\n",
    "    Row(id=4, name=\"Max\", salary=90000, managerId=None)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data)\n",
    "\n",
    "# Register temp view\n",
    "employee_df.createOrReplaceTempView(\"Employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "122882f6-c393-4545-a662-f8c8e49d92eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employee = employee_df.selectExpr(\"id as emp_id \", \"name as emp_name\",\"salary as emp_salary\",\"managerId as emp_managerId\")\n",
    "df_manager = employee_df.selectExpr(\"id as mgr_id \", \"name as mgr_name\",\"salary as mgr_salary\",\"managerId as mgr_managerId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89a45c2-1c49-4425-94b3-8f1dd7dac1fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employee.join(df_manager,df_employee[\"emp_managerId\"] == df_manager[\"mgr_id\"],how =\"inner\")\\\n",
    "    .filter(df_employee[\"emp_salary\"] > df_manager[\"mgr_salary\"])\\\n",
    "    .selectExpr(\"emp_name as Employee\")\\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8745e7b0-507c-4114-b6f7-dabf82497fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "```markdown\n",
    "In PySpark, you can reference and manipulate columns in several expressive ways depending on the contextâ€”whether you're selecting, transforming, filtering, or aliasing. Here's a unified cheat sheet that captures the **different ways to write and use columns in PySpark**:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  PySpark Column Syntax Cheat Sheet\n",
    "\n",
    "### ðŸ”¹ 1. **Dot Notation**\n",
    "```python\n",
    "df.colName\n",
    "df.salary\n",
    "```\n",
    "- Simple and readable.\n",
    "- Works only if `colName` is a valid Python identifier (no spaces or special characters).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. **Bracket Notation**\n",
    "```python\n",
    "df[\"colName\"]\n",
    "df[\"salary\"]\n",
    "```\n",
    "- More flexible.\n",
    "- Useful when column names have spaces, dots, or special characters.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. **`col()` Function**\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "col(\"salary\")\n",
    "col(\"employee.name\")\n",
    "```\n",
    "- Preferred in transformations and filters.\n",
    "- Enables chaining like `.alias()`, `.cast()`, `.substr()`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. **`df.selectExpr()` with SQL Expressions**\n",
    "```python\n",
    "df.selectExpr(\"salary * 1.1 as updated_salary\", \"name\")\n",
    "```\n",
    "- Powerful for inline calculations and aliasing.\n",
    "- Accepts SQL-like strings.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. **`df.select()` with Column Objects**\n",
    "```python\n",
    "df.select(col(\"salary\").alias(\"updated_salary\"), col(\"name\"))\n",
    "```\n",
    "- Explicit and readable.\n",
    "- Great for chaining transformations.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 6. **`df.withColumn()` for Adding or Modifying Columns**\n",
    "```python\n",
    "df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
    "```\n",
    "- Adds or replaces a column.\n",
    "- Often used in pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 7. **`df[\"colName\"].alias(\"newName\")`**\n",
    "```python\n",
    "df.select(df[\"salary\"].alias(\"updated_salary\"))\n",
    "```\n",
    "- Combines bracket notation with aliasing.\n",
    "- Handy in joins or renaming.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 8. **SQL via Temp View**\n",
    "```python\n",
    "df.createOrReplaceTempView(\"Employee\")\n",
    "spark.sql(\"SELECT name, salary FROM Employee WHERE salary > 50000\")\n",
    "```\n",
    "- Ideal for SQL lovers.\n",
    "- Column access via SQL syntax.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 9. **Nested Struct Columns**\n",
    "```python\n",
    "df.select(\"employee.name\", \"employee.salary\")\n",
    "col(\"employee.name\")\n",
    "```\n",
    "- For accessing fields inside structs.\n",
    "- Use dot notation or `col()`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§ª Bonus: Column Functions\n",
    "You can apply transformations directly:\n",
    "```python\n",
    "col(\"name\").substr(1, 3)\n",
    "col(\"salary\").cast(\"double\")\n",
    "col(\"name\").like(\"J%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like this wrapped into a visual cheat sheet or added to your DataGym onboarding module? I can also generate examples for each style using your favorite dataset.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cfefd2f-ac14-42ec-834a-91b61384df9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "181",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
