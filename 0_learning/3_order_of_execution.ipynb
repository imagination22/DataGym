{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8723cd8d-dbe2-462c-aa83-a888fcc08c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🧠 SQL Query Execution Order — Complete & Annotated Guide\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Execution Flow\n",
    "\n",
    "```text\n",
    "WITH → FROM → JOIN → ON → WHERE → GROUP BY → ROLLUP/CUBE/GROUPING SETS → HAVING → WINDOW → SELECT → PIVOT/UNPIVOT → CASE → DISTINCT → UNION/INTERSECT/EXCEPT → ORDER BY → OFFSET → FETCH/LIMIT\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. `WITH` (Common Table Expressions)\n",
    "\n",
    "- Defines temporary named result sets\n",
    "- Executed first—even before `FROM`\n",
    "\n",
    "```sql\n",
    "WITH top_sellers AS (\n",
    "  SELECT seller_id, SUM(sales) AS total_sales\n",
    "  FROM orders\n",
    "  GROUP BY seller_id\n",
    ")\n",
    "SELECT * FROM top_sellers WHERE total_sales > 10000;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. `FROM`\n",
    "\n",
    "- Identifies source tables or subqueries\n",
    "\n",
    "```sql\n",
    "SELECT * FROM employees;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. `JOIN` + `ON`\n",
    "\n",
    "- Combines rows from multiple tables\n",
    "\n",
    "```sql\n",
    "SELECT e.name, d.name\n",
    "FROM employees e\n",
    "JOIN departments d ON e.dept_id = d.id;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. `WHERE`\n",
    "\n",
    "- Filters rows before aggregation\n",
    "\n",
    "```sql\n",
    "SELECT * FROM orders WHERE status = 'shipped';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 5. `GROUP BY`\n",
    "\n",
    "- Aggregates rows into groups\n",
    "\n",
    "```sql\n",
    "SELECT dept_id, COUNT(*) FROM employees GROUP BY dept_id;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 6. `ROLLUP`, `CUBE`, `GROUPING SETS`\n",
    "\n",
    "- Multi-level aggregation and dimensional summaries\n",
    "\n",
    "```sql\n",
    "SELECT region, product, SUM(sales)\n",
    "FROM orders\n",
    "GROUP BY ROLLUP(region, product);\n",
    "```\n",
    "\n",
    "```sql\n",
    "SELECT region, product, SUM(sales)\n",
    "FROM orders\n",
    "GROUP BY CUBE(region, product);\n",
    "```\n",
    "\n",
    "```sql\n",
    "SELECT region, product, SUM(sales)\n",
    "FROM orders\n",
    "GROUP BY GROUPING SETS (\n",
    "  (region, product),\n",
    "  (region),\n",
    "  ()\n",
    ");\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 7. `HAVING`\n",
    "\n",
    "- Filters groups after aggregation\n",
    "\n",
    "```sql\n",
    "SELECT dept_id, COUNT(*) AS total\n",
    "FROM employees\n",
    "GROUP BY dept_id\n",
    "HAVING COUNT(*) > 10;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 8. `WINDOW` (Analytic Functions)\n",
    "\n",
    "- Calculates values over partitions without collapsing rows\n",
    "\n",
    "```sql\n",
    "SELECT name, salary,\n",
    "       RANK() OVER (PARTITION BY dept_id ORDER BY salary DESC) AS rank\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 9. `SELECT`\n",
    "\n",
    "- Projects columns and expressions\n",
    "\n",
    "```sql\n",
    "SELECT name, salary FROM employees;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 10. `PIVOT` / `UNPIVOT`\n",
    "\n",
    "- Reshapes data between wide and tall formats\n",
    "\n",
    "```sql\n",
    "-- PIVOT (SQL Server)\n",
    "SELECT *\n",
    "FROM (\n",
    "  SELECT year, region, sales FROM orders\n",
    ") AS src\n",
    "PIVOT (\n",
    "  SUM(sales) FOR region IN ([North], [South], [East], [West])\n",
    ") AS pvt;\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- UNPIVOT\n",
    "SELECT year, region, sales\n",
    "FROM (\n",
    "  SELECT year, North, South, East, West FROM sales_summary\n",
    ") AS src\n",
    "UNPIVOT (\n",
    "  sales FOR region IN (North, South, East, West)\n",
    ") AS unpvt;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 11. `CASE`\n",
    "\n",
    "- Applies conditional logic during projection\n",
    "\n",
    "```sql\n",
    "SELECT name,\n",
    "       CASE\n",
    "         WHEN salary > 100000 THEN 'High'\n",
    "         WHEN salary > 50000 THEN 'Medium'\n",
    "         ELSE 'Low'\n",
    "       END AS salary_band\n",
    "FROM employees;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 12. `DISTINCT`\n",
    "\n",
    "- Removes duplicate rows from result\n",
    "\n",
    "```sql\n",
    "SELECT DISTINCT dept_id FROM employees;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 13. `UNION` / `INTERSECT` / `EXCEPT`\n",
    "\n",
    "- Combines or compares result sets vertically\n",
    "\n",
    "```sql\n",
    "SELECT name FROM customers\n",
    "UNION\n",
    "SELECT name FROM vendors;\n",
    "\n",
    "SELECT name FROM customers\n",
    "INTERSECT\n",
    "SELECT name FROM vendors;\n",
    "\n",
    "SELECT name FROM customers\n",
    "EXCEPT\n",
    "SELECT name FROM vendors;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 14. `ORDER BY`\n",
    "\n",
    "- Sorts final result set\n",
    "\n",
    "```sql\n",
    "SELECT name FROM employees ORDER BY salary DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 15. `OFFSET` / `FETCH` / `LIMIT`\n",
    "\n",
    "- Applies pagination\n",
    "\n",
    "```sql\n",
    "SELECT name FROM employees ORDER BY salary DESC OFFSET 10 ROWS FETCH NEXT 5 ROWS ONLY;\n",
    "-- or in MySQL/PostgreSQL\n",
    "SELECT name FROM employees ORDER BY salary DESC LIMIT 5 OFFSET 10;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Final Visual Summary\n",
    "\n",
    "| Step | Clause                      | Purpose                              |\n",
    "|------|-----------------------------|--------------------------------------|\n",
    "| 1    | `WITH`                      | Define temporary result sets         |\n",
    "| 2    | `FROM`                      | Identify source tables               |\n",
    "| 3    | `JOIN` + `ON`               | Combine tables                       |\n",
    "| 4    | `WHERE`                     | Filter rows                          |\n",
    "| 5    | `GROUP BY`                  | Aggregate rows                       |\n",
    "| 6    | `ROLLUP/CUBE/GROUPING SETS` | Multi-level aggregation              |\n",
    "| 7    | `HAVING`                    | Filter groups                        |\n",
    "| 8    | `WINDOW`                    | Compute analytics                    |\n",
    "| 9    | `SELECT`                    | Project columns                      |\n",
    "| 10   | `PIVOT/UNPIVOT`             | Reshape data                         |\n",
    "| 11   | `CASE`                      | Apply conditional logic              |\n",
    "| 12   | `DISTINCT`                  | Remove duplicates                    |\n",
    "| 13   | `cast`                      | cast operations                      |\n",
    "| 13   | `alias`                     | Alias operations                     |\n",
    "| 13   | `UNION/INTERSECT/EXCEPT`    | Set operations                       |\n",
    "| 14   | `ORDER BY`                  | Sort results                         |\n",
    "| 15   | `OFFSET/FETCH`              | Paginate results                     |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "%md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fcaf9e2-2dc6-4909-8783-fe17fb6b22e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🧠 SQL vs PySpark Keyword Comparison — Unified Cheat Sheet\n",
    "\n",
    "This guide maps each SQL clause or keyword to its PySpark DataFrame API equivalent, with usage notes and examples. Perfect for learners, architects, and DataGym contributors.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 SQL vs PySpark Mapping Table\n",
    "\n",
    "| SQL Keyword              | PySpark Equivalent                             | Purpose / Notes                                                                 | Example                                                  |\n",
    "|--------------------------|------------------------------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------|\n",
    "| `SELECT`                 | `df.select(...)`                               | Column projection                                                               | `df.select(\"name\", \"salary\")`                           |\n",
    "| `FROM`                   | `spark.read`, `spark.table`, `df`              | Load data from source                                                           | `spark.read.csv(\"file.csv\")`                            |\n",
    "| `WHERE`                  | `df.filter(...)`                               | Row-level filtering                                                             | `df.filter(\"age > 30\")`                                 |\n",
    "| `JOIN`                   | `df.join(df2, on=..., how=...)`                | Combine rows from multiple DataFrames                                           | `df1.join(df2, \"id\", \"inner\")`                          |\n",
    "| `ON`                     | `on=` parameter in `.join()`                   | Join condition                                                                  | `df1.join(df2, df1.id == df2.id)`                       |\n",
    "| `GROUP BY`               | `df.groupBy(...).agg(...)`                     | Aggregate rows                                                                  | `df.groupBy(\"dept\").agg(F.sum(\"salary\"))`              |\n",
    "| `HAVING`                 | `.filter(...)` after `.groupBy().agg()`        | Filter aggregated results                                                       | `df.groupBy(...).agg(...).filter(\"sum > 1000\")`        |\n",
    "| `ORDER BY`               | `df.orderBy(...)`                              | Sort result set                                                                 | `df.orderBy(\"salary\", ascending=False)`                |\n",
    "| `LIMIT` / `OFFSET`       | `df.limit(n)`                                  | Limit number of rows                                                            | `df.limit(10)`                                          |\n",
    "| `DISTINCT`               | `df.distinct()`                                | Remove duplicate rows                                                           | `df.select(\"dept\").distinct()`                         |\n",
    "| `UNION` / `UNION ALL`    | `df1.union(df2)`                               | Combine result sets vertically                                                  | `df1.union(df2)`                                        |\n",
    "| `EXCEPT`                 | `df1.exceptAll(df2)`                           | Rows in `df1` not in `df2`                                                      | `df1.exceptAll(df2)`                                    |\n",
    "| `INTERSECT`              | `df1.intersect(df2)`                           | Common rows between `df1` and `df2`                                             | `df1.intersect(df2)`                                    |\n",
    "| `WITH` (CTE)             | `df.createOrReplaceTempView(\"name\")`           | Define temporary views                                                          | `df.createOrReplaceTempView(\"sales_view\")`             |\n",
    "| `CASE`                   | `F.when(...).otherwise(...)`                  | Conditional logic                                                               | `df.withColumn(\"band\", F.when(...).otherwise(...))`    |\n",
    "| `WINDOW` Functions       | `Window.partitionBy(...).orderBy(...)`         | Analytic functions (rank, row_number, etc.)                                     | `F.rank().over(Window.partitionBy(...))`               |\n",
    "| `ROLLUP`                 | `df.rollup(...).agg(...)`                      | Hierarchical aggregation                                                        | `df.rollup(\"region\", \"product\").agg(F.sum(\"sales\"))`   |\n",
    "| `CUBE`                   | `df.cube(...).agg(...)`                        | Cross-dimensional aggregation                                                   | `df.cube(\"region\", \"product\").agg(F.sum(\"sales\"))`     |\n",
    "| `GROUPING SETS`          | `df.groupingSets([...]).agg(...)`              | Custom group combinations                                                       | `df.groupingSets([[\"region\"], [\"product\"]])`           |\n",
    "| `PIVOT`                  | `df.groupBy(...).pivot(...).agg(...)`          | Convert rows to columns                                                         | `df.groupBy(\"year\").pivot(\"region\").sum(\"sales\")`      |\n",
    "| `UNPIVOT`                | ❌ Not directly supported (requires melt logic) | Convert columns to rows (manual workaround using `explode`, `stack`, etc.)      | Use `selectExpr(\"stack(...)\")`                         |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 Notes on Execution\n",
    "\n",
    "- PySpark uses **lazy evaluation**: transformations are staged until an action like `.show()`, `.collect()`, or `.write()` is triggered.\n",
    "- The **Catalyst optimizer** may **reorder operations** for performance (e.g., push filters before joins).\n",
    "- Some SQL features like `UNPIVOT`, `OFFSET`, and recursive CTEs require **manual workarounds** in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 SQL vs PySpark — Side-by-Side Example\n",
    "\n",
    "### 🧾 SQL\n",
    "```sql\n",
    "SELECT region, product, SUM(sales) AS total_sales\n",
    "FROM orders\n",
    "WHERE status = 'shipped'\n",
    "GROUP BY CUBE(region, product)\n",
    "HAVING SUM(sales) > 10000\n",
    "ORDER BY total_sales DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "### 🧪 PySpark\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result = (\n",
    "    df.filter(\"status = 'shipped'\")\n",
    "      .cube(\"region\", \"product\")\n",
    "      .agg(F.sum(\"sales\").alias(\"total_sales\"))\n",
    "      .filter(\"total_sales > 10000\")\n",
    "      .orderBy(F.desc(\"total_sales\"))\n",
    "      .limit(10)\n",
    ")\n",
    "result.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Key Differences\n",
    "\n",
    "- **SQL is declarative**: You describe what you want.\n",
    "- **PySpark is procedural**: You chain transformations.\n",
    "- **SQL execution order is fixed**: Spark’s optimizer may reorder operations for performance.\n",
    "- **CTEs (`WITH`)** are not natively supported in PySpark SQL—you use temp views instead.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Bonus: How Spark Executes It\n",
    "\n",
    "1. **Unresolved Logical Plan**: Based on your code.\n",
    "2. **Resolved Logical Plan**: Spark resolves column names and types.\n",
    "3. **Optimized Logical Plan**: Catalyst applies rules (e.g., predicate pushdown).\n",
    "4. **Physical Plan**: Spark decides how to execute (e.g., shuffle, broadcast).\n",
    "5. **Execution**: Spark runs the plan across the cluster.\n",
    "\n",
    "Inspect it using:\n",
    "\n",
    "```python\n",
    "df.explain(True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "📘 *Want this turned into a printable cheat sheet, interactive notebook, or visual guide for DataGym? I’d love to help you build it!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396792d8-b868-4096-9ad7-151d88628c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🧠 PySpark for SQL Pros: Transition Guide\n",
    "\n",
    "This guide helps you bridge the gap between SQL’s declarative style and PySpark’s transformation-first, distributed mindset. It highlights what feels familiar, what behaves differently, and where PySpark offers unique superpowers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 What Feels Familiar (SQL-like in PySpark)\n",
    "\n",
    "| SQL Concept         | PySpark Equivalent             | Notes |\n",
    "|---------------------|--------------------------------|-------|\n",
    "| `SELECT`            | `df.select(...)`               | Column projection |\n",
    "| `WHERE`             | `df.filter(...)`               | Row-level filtering |\n",
    "| `GROUP BY` + `HAVING` | `df.groupBy().agg().filter()` | Aggregation + post-filter |\n",
    "| `ORDER BY`          | `df.orderBy(...)`              | Sorting |\n",
    "| `JOIN`              | `df.join(...)`                 | Supports all join types |\n",
    "| `CASE`              | `F.when(...).otherwise(...)`   | Conditional logic |\n",
    "| `DISTINCT`          | `df.distinct()`                | Removes duplicates |\n",
    "| `UNION`             | `df.union(...)`                | Vertical merge |\n",
    "| `LIMIT`             | `df.limit(n)`                  | Row slicing |\n",
    "\n",
    "✅ These will feel intuitive—you’re just swapping SQL syntax for PySpark methods.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ What Behaves Differently (Surprise Zones)\n",
    "\n",
    "| PySpark Feature         | SQL Equivalent | Surprise Factor |\n",
    "|-------------------------|----------------|-----------------|\n",
    "| `withColumn()`          | ❌             | Add/transform columns dynamically |\n",
    "| `withColumnRenamed()`   | ❌             | Rename columns outside `SELECT` |\n",
    "| `selectExpr()`          | ❌             | SQL-like expressions in strings |\n",
    "| `explode()`             | ❌             | Flatten arrays/maps into rows |\n",
    "| `split()`               | ❌             | Turns strings into arrays |\n",
    "| `array()`, `map()`, `struct()` | ❌     | Constructs complex types |\n",
    "| `get_json_object()`     | ❌             | Parses JSON strings inline |\n",
    "| `drop()`                | ❌             | Removes columns mid-pipeline |\n",
    "| `alias()`               | ✅             | More flexible in PySpark |\n",
    "| `cast()`                | ✅             | Same logic, different syntax |\n",
    "| `filter()` chaining     | ✅             | Procedural, not declarative |\n",
    "| `Window` functions      | ✅             | Requires explicit `WindowSpec` |\n",
    "| `UDFs` / `Pandas UDFs`  | ❌             | Custom Python logic on columns |\n",
    "\n",
    "💡 These are **not available in SQL**, and they’re where PySpark becomes a **data engineering toolkit**, not just a query language.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Types of Transformations in PySpark\n",
    "\n",
    "### 1. **Narrow Transformations**\n",
    "- Operate on a single partition\n",
    "- No shuffle required\n",
    "- ✅ Fast and efficient\n",
    "\n",
    "Examples:\n",
    "```python\n",
    "df.select(\"name\")\n",
    "df.filter(\"age > 30\")\n",
    "df.withColumn(\"discount\", F.col(\"price\") * 0.9)\n",
    "```\n",
    "\n",
    "### 2. **Wide Transformations**\n",
    "- Require data movement across partitions\n",
    "- Trigger shuffles\n",
    "- ⚠️ Expensive if not optimized\n",
    "\n",
    "Examples:\n",
    "```python\n",
    "df.groupBy(\"dept\").agg(F.sum(\"salary\"))\n",
    "df.join(other_df, \"id\")\n",
    "df.orderBy(\"salary\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Execution Model Differences\n",
    "\n",
    "| Concept               | SQL Engine                  | PySpark Engine                     |\n",
    "|-----------------------|-----------------------------|------------------------------------|\n",
    "| Evaluation            | Declarative                 | Lazy (until `.show()`, `.collect()`) |\n",
    "| Optimization          | Rule-based + cost-based     | Catalyst optimizer + DAG scheduler |\n",
    "| Execution             | Single-node or MPP          | Distributed across cluster         |\n",
    "| Schema enforcement    | Strict                      | Flexible, inferred or defined      |\n",
    "| Error handling        | Immediate                   | Deferred until action              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Does Execution Order Matter in PySpark?\n",
    "\n",
    "### ✅ Yes—but differently than SQL.\n",
    "\n",
    "### 🔹 In SQL:\n",
    "- Execution order is **fixed and logical**.\n",
    "- You write `SELECT` first, but it’s evaluated after `FROM`, `JOIN`, `WHERE`, etc.\n",
    "- The engine reorders and optimizes internally.\n",
    "\n",
    "### 🔸 In PySpark:\n",
    "- You **chain transformations** in procedural order.\n",
    "- Each step **depends on the previous one**.\n",
    "- Spark builds a **logical plan**, then optimizes it—but doesn’t rewrite your logic unless safe.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Examples of Chaining Sensitivity\n",
    "\n",
    "### ✅ Correct:\n",
    "```python\n",
    "df.withColumn(\"discounted\", F.col(\"price\") * 0.9)\n",
    "  .filter(F.col(\"discounted\") > 100)\n",
    "```\n",
    "\n",
    "### ❌ Incorrect:\n",
    "```python\n",
    "df.filter(F.col(\"discounted\") > 100)  # 'discounted' doesn't exist yet!\n",
    "```\n",
    "\n",
    "### ⚠️ Semantic Shift:\n",
    "```python\n",
    "df.orderBy(\"salary\").limit(10)  # Sort full dataset, then take top 10\n",
    "df.limit(10).orderBy(\"salary\")  # Sort only first 10 rows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Summary: SQL vs PySpark Execution Logic\n",
    "\n",
    "| Aspect                     | SQL                          | PySpark                          |\n",
    "|----------------------------|------------------------------|----------------------------------|\n",
    "| Execution order            | Fixed (logical)              | Procedural (chained)             |\n",
    "| Optimizer reordering       | Aggressive                   | Conservative                     |\n",
    "| Dependency between steps   | Abstracted                   | Explicit                         |\n",
    "| Mistakes from wrong order  | Rare (SQL engine corrects)   | Common (breaks or misbehaves)    |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Transition Tips\n",
    "\n",
    "- ✅ Think in **transformations**, not queries.\n",
    "- ✅ Use `df.explain(True)` to inspect plans.\n",
    "- ✅ Cache intermediate results if reused.\n",
    "- ✅ Prefer built-in functions (`F.*`) over UDFs for performance.\n",
    "- ✅ Watch out for **wide transformations**—they’re your new bottlenecks.\n",
    "\n",
    "---\n",
    "\n",
    "📘 *Want this turned into a DataGym onboarding module, a side-by-side SQL vs PySpark notebook, or a printable cheat sheet for contributors? I’d love to help you build it!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce91ef6-0aef-4611-ab2e-479dcc3953cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_order_of_execution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
