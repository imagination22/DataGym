{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9c49bb-5202-45d2-a23b-9d884f83d208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Entire PySpark column syntax guide wrapped inside a single code block so you can paste it directly into a **Databricks Markdown cell**â€”clean, complete, and ready for DataGym documentation or onboarding notebooks:\n",
    "\n",
    "\n",
    "# ðŸ§  PySpark Column Syntax Cheat Sheet â€” Reference, Transform, Alias\n",
    "\n",
    "In PySpark, you can reference and manipulate columns in several expressive ways depending on the contextâ€”whether you're selecting, transforming, filtering, or aliasing. This unified guide captures the most common and powerful patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Dot Notation\n",
    "\n",
    "```python\n",
    "df.colName\n",
    "df.salary\n",
    "```\n",
    "\n",
    "- Simple and readable.\n",
    "- Works only if `colName` is a valid Python identifier (no spaces or special characters).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Bracket Notation\n",
    "\n",
    "```python\n",
    "df[\"colName\"]\n",
    "df[\"salary\"]\n",
    "```\n",
    "\n",
    "- More flexible.\n",
    "- Useful when column names have spaces, dots, or special characters.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. `col()` Function\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "col(\"salary\")\n",
    "col(\"employee.name\")\n",
    "```\n",
    "\n",
    "- Preferred in transformations and filters.\n",
    "- Enables chaining like `.alias()`, `.cast()`, `.substr()`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. `df.selectExpr()` with SQL Expressions\n",
    "\n",
    "```python\n",
    "df.selectExpr(\"salary * 1.1 as updated_salary\", \"name\")\n",
    "```\n",
    "\n",
    "- Powerful for inline calculations and aliasing.\n",
    "- Accepts SQL-like strings.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. `df.select()` with Column Objects\n",
    "\n",
    "```python\n",
    "df.select(col(\"salary\").alias(\"updated_salary\"), col(\"name\"))\n",
    "```\n",
    "\n",
    "- Explicit and readable.\n",
    "- Great for chaining transformations.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6. `df.withColumn()` for Adding or Modifying Columns\n",
    "\n",
    "```python\n",
    "df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
    "```\n",
    "\n",
    "- Adds or replaces a column.\n",
    "- Often used in pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 7. `df[\"colName\"].alias(\"newName\")`\n",
    "\n",
    "```python\n",
    "df.select(df[\"salary\"].alias(\"updated_salary\"))\n",
    "```\n",
    "\n",
    "- Combines bracket notation with aliasing.\n",
    "- Handy in joins or renaming.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 8. SQL via Temp View\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"Employee\")\n",
    "spark.sql(\"SELECT name, salary FROM Employee WHERE salary > 50000\")\n",
    "```\n",
    "\n",
    "- Ideal for SQL lovers.\n",
    "- Column access via SQL syntax.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 9. Nested Struct Columns\n",
    "\n",
    "```python\n",
    "df.select(\"employee.name\", \"employee.salary\")\n",
    "col(\"employee.name\")\n",
    "```\n",
    "\n",
    "- For accessing fields inside structs.\n",
    "- Use dot notation or `col()`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Bonus: Column Functions\n",
    "\n",
    "You can apply transformations directly:\n",
    "\n",
    "```python\n",
    "col(\"name\").substr(1, 3)\n",
    "col(\"salary\").cast(\"double\")\n",
    "col(\"name\").like(\"J%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e9a5145-db0b-471e-afac-7143f6cae73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark column access and transformation cheatsheet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
