{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61472851-1c44-4966-84bd-37f541f030b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1211. Queries Quality and Percentage\n",
    "## Table: Queries\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| query_name  | varchar |\n",
    "| result      | varchar |\n",
    "| position    | int     |\n",
    "| rating      | int     |\n",
    "\n",
    "### Primary Key:\n",
    "This table may have duplicate rows.\n",
    "\n",
    "### Column Descriptions:\n",
    "- `query_name`: Name of the query.\n",
    "- `result`: Result returned by the query.\n",
    "- `position`: Position of the result in the list (1 to 500).\n",
    "- `rating`: Rating of the result (1 to 5). Query with rating less than 3 is a poor query.\n",
    "\n",
    "---\n",
    "\n",
    "This table may have duplicate rows.  \n",
    "This table contains information collected from some queries on a database.  \n",
    "The position column has a value from 1 to 500.  \n",
    "The rating column has a value from 1 to 5. Query with rating less than 3 is a poor query.\n",
    "\n",
    "We define query quality as:\n",
    "\n",
    "The average of the ratio between query rating and its position.\n",
    "\n",
    "We also define poor query percentage as:\n",
    "\n",
    "The percentage of all queries with rating less than 3.\n",
    "\n",
    "Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "\n",
    "Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1:\n",
    "\n",
    "**Input:**\n",
    "\n",
    "Queries table:  \n",
    "| query_name | result            | position | rating |\n",
    "|------------|-------------------|----------|--------|\n",
    "| Dog        | Golden Retriever  | 1        | 5      |\n",
    "| Dog        | German Shepherd   | 2        | 5      |\n",
    "| Dog        | Mule              | 200      | 1      |\n",
    "| Cat        | Shirazi           | 5        | 2      |\n",
    "| Cat        | Siamese           | 3        | 3      |\n",
    "| Cat        | Sphynx            | 7        | 4      |\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| query_name | quality | poor_query_percentage |\n",
    "|------------|---------|-----------------------|\n",
    "| Dog        | 2.50    | 33.33                 |\n",
    "| Cat        | 0.66    | 33.33                 |\n",
    "\n",
    "**Explanation:**  \n",
    "Dog queries quality is ((5 / 1) + (5 / 2) + (1 / 200)) / 3 = 2.50  \n",
    "Dog queries poor_query_percentage is (1 / 3) * 100 = 33.33  \n",
    "\n",
    "Cat queries quality equals ((2 / 5) + (3 / 3) + (4 / 7)) / 3 = 0.66  \n",
    "Cat queries poor_query_percentage is (1 / 3) * 100 = 33.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b666c04e-7070-416a-a34a-06cb15b31478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, avg, round, count, sum, when\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"query_name\", StringType(), True),\n",
    "    StructField(\"result\", StringType(), True),\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Dog\", \"Golden Retriever\", 1, 5),\n",
    "    (\"Dog\", \"German Shepherd\", 2, 5),\n",
    "    (\"Dog\", \"Mule\", 200, 1),\n",
    "    (\"Cat\", \"Shirazi\", 5, 2),\n",
    "    (\"Cat\", \"Siamese\", 3, 3),\n",
    "    (\"Cat\", \"Sphynx\", 7, 4)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"Queries\")\n",
    "\n",
    "# SQL logic\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        query_name,\n",
    "        ROUND(AVG(rating * 1.0 / position), 2) AS quality,\n",
    "        ROUND(SUM(CASE WHEN rating < 3 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS poor_query_percentage\n",
    "    FROM Queries\n",
    "    GROUP BY query_name\n",
    "\"\"\").createOrReplaceTempView(\"QueryStats\")\n",
    "\n",
    "# Display result\n",
    "display(spark.sql(\"SELECT * FROM QueryStats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9704f70-78b8-49d6-9c17-72647eb753f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_agg = (\n",
    "    df.groupBy(\"query_name\")\n",
    "    .agg(\n",
    "        (avg(col(\"rating\") * 1.0 / col(\"position\")).cast(\"decimal(5,2)\")).alias(\n",
    "            \"quality\"\n",
    "        ),\n",
    "        (\n",
    "            (sum(when(col(\"rating\") < 3, 1).otherwise(0)) * 100.0 / count(\"*\")).cast(\n",
    "                \"decimal(5,2)\"\n",
    "            )\n",
    "        ).alias(\"poor_query_percentage\"),\n",
    "    )\n",
    "    .display()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f2b7ed-3773-4a31-b152-e6182eefa8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Great question, Shubham â€” you're close, but there are a few subtle mistakes in your PySpark chain. Letâ€™s break it down and fix it step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ Mistakes in Your Code\n",
    "\n",
    "```python\n",
    "df_2 = df.groupBy(col(\"query_name\")).agg(\n",
    "    avg(1.0 * col(\"rating\") / col(\"position\")).alias(\"quality\").cast(\"decimal(4,2)\")\n",
    ").withColumn(\n",
    "    \"poor_query_percentage\", \n",
    "    sum(when(col(\"rating\") < 3, 1).otherwise 0 )/count(*)\n",
    ").display()\n",
    "```\n",
    "\n",
    "#### ðŸ”´ Mistake 1: `.cast()` is applied **after** `.alias()`\n",
    "- Youâ€™re trying to cast the result of `avg(...)` **after** aliasing it, which doesnâ€™t work.\n",
    "- `.cast()` must be applied **before** `.alias()`.\n",
    "\n",
    "#### ðŸ”´ Mistake 2: `otherwise 0` is missing parentheses\n",
    "- `otherwise` is a method â€” it needs to be called like `otherwise(0)`.\n",
    "\n",
    "#### ðŸ”´ Mistake 3: `count(*)` is invalid in PySpark\n",
    "- You need to use `count(\"*\")` as a string or `count(col(\"*\"))`.\n",
    "\n",
    "#### ðŸ”´ Mistake 4: `withColumn()` after `groupBy().agg()` wonâ€™t work as expected\n",
    "- After aggregation, the original columns like `rating` are no longer available.\n",
    "- You need to compute both metrics **inside the same `agg()` block**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Corrected Version\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, avg, sum, count, when, round\n",
    "\n",
    "df_2 = df.groupBy(\"query_name\").agg(\n",
    "    round(avg(col(\"rating\") * 1.0 / col(\"position\")), 2).alias(\"quality\"),\n",
    "    round(\n",
    "        sum(when(col(\"rating\") < 3, 1).otherwise(0)) * 100.0 / count(\"*\"),\n",
    "        2\n",
    "    ).alias(\"poor_query_percentage\")\n",
    ")\n",
    "\n",
    "display(df_2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Pro Tip\n",
    "If you really want to cast to `DecimalType(4,2)` instead of rounding, you can use:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "(col(\"rating\") * 1.0 / col(\"position\")).cast(DecimalType(4,2))\n",
    "```\n",
    "\n",
    "But for most reporting and display purposes, `round(..., 2)` is cleaner and safer.\n",
    "\n",
    "Let me know if you want this wrapped into a reusable function or added to a pipeline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a54f46f-5cc4-4ec5-820c-659bfe4b4c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Thanks for sharing the full error, Shubham â€” this is a classic precision mismatch in PySpark when casting floating-point values to a `DecimalType`.\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Root Cause\n",
    "\n",
    "You're trying to cast a `Double` (from `avg(...)` or a division) into `Decimal(3,2)`, which means:\n",
    "- **3 total digits**, **2 after the decimal** â†’ max value allowed is `9.99`.\n",
    "\n",
    "But your computed value (e.g., `quality = 2.50`, `poor_query_percentage = 33.33`) **exceeds** that range â€” especially `33.33`, which needs at least `Decimal(5,2)`.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Fix Options\n",
    "\n",
    "#### Option 1: Use a wider decimal type\n",
    "```python\n",
    ".cast(\"decimal(5,2)\")\n",
    "```\n",
    "This allows values up to `999.99`, which safely covers your use case.\n",
    "\n",
    "#### Option 2: Use `round()` instead of `cast()`\n",
    "```python\n",
    "round(avg(...), 2)\n",
    "```\n",
    "This avoids precision errors and is more display-friendly.\n",
    "\n",
    "#### Option 3: Disable ANSI mode (not recommended unless you're okay with silent nulls)\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "```\n",
    "This will suppress the error and return `null` for out-of-range casts â€” but can hide bugs.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Recommended Fix\n",
    "\n",
    "Hereâ€™s your corrected snippet:\n",
    "\n",
    "```python\n",
    "df_agg = df.groupBy(\"query_name\").agg(\n",
    "    (avg(col(\"rating\") * 1.0 / col(\"position\")).cast(\"decimal(5,2)\")).alias(\"quality\"),\n",
    "    ((sum(when(col(\"rating\") < 3, 1).otherwise(0)) * 100.0 / count(\"*\")).cast(\"decimal(5,2)\")).alias(\"poor_query_percentage\")\n",
    ")\n",
    "\n",
    "display(df_agg)\n",
    "```\n",
    "\n",
    "Let me know if you want to format the output as strings like `\"2.50\"` or `\"33.33\"` for presentation or export!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eeace43-6f58-4288-b5b4-02fb40b06116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1211_leetcode_easy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
