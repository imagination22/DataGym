{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea57011-3421-43b4-8eb6-89b5b7601a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.format('json').option('inferSchema', True)\\\n",
    "                    .option('header', True)\\\n",
    "                    .option('multiLine', False)\\\n",
    "                    .load('/Volumes/pyspark_practice/default/files/Practice/drivers.json')\n",
    "df_json.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ed86d7-cf3e-4b8b-9dda-d7eca0d0fd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß† Unity Catalog Assets ‚Äî Volumes, Tables, and Models Explained\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ What Is a Volume?\n",
    "\n",
    "A **volume** in Unity Catalog is a governed storage container for **non-tabular data**‚Äîthink files like images, logs, PDFs, JSON, or ML datasets. It‚Äôs designed to complement tables by handling unstructured and semi-structured data.\n",
    "\n",
    "### üîπ Is a Volume Created Inside a Schema?\n",
    "\n",
    "Yes! In Unity Catalog, volumes are **siblings** to tables, views, and models. They live inside a **schema**, which itself belongs to a **catalog**.\n",
    "\n",
    "> üìÅ Path format: `/Volumes/<catalog>/<schema>/<volume>/<path>/<file>`\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Types of Volumes\n",
    "\n",
    "### 1. **Managed Volume**\n",
    "- Created and fully governed by Unity Catalog.\n",
    "- No need to specify a location‚ÄîDatabricks handles storage.\n",
    "- File access is only through Unity Catalog paths.\n",
    "- Deleted volumes retain files for 7 days before cleanup.\n",
    "\n",
    "‚úÖ Use when:\n",
    "- You want simple governance.\n",
    "- You don‚Äôt need external access to the files.\n",
    "\n",
    "‚ö†Ô∏è Avoid when:\n",
    "- You need direct cloud URI access or external system integration.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **External Volume**\n",
    "- Points to a directory in your cloud storage (e.g., S3, ADLS).\n",
    "- You specify the location during creation.\n",
    "- Unity Catalog governs access, but external systems can still read/write directly.\n",
    "\n",
    "‚úÖ Use when:\n",
    "- You already have data in cloud storage.\n",
    "- You want to apply governance without migrating files.\n",
    "\n",
    "‚ö†Ô∏è Avoid when:\n",
    "- You want Unity Catalog to fully manage lifecycle and cleanup.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä What Is a Table?\n",
    "\n",
    "A **table** is a structured dataset with rows and columns. In Unity Catalog, tables are governed objects inside schemas.\n",
    "\n",
    "### üîπ Types of Tables\n",
    "\n",
    "| Type           | Description                              | Use Case                          |\n",
    "|----------------|------------------------------------------|-----------------------------------|\n",
    "| Managed Table  | Databricks manages both data & metadata  | Internal-only workflows           |\n",
    "| External Table | Data lives in external storage           | Shared or multi-tool environments |\n",
    "| Delta Table    | Transactional, versioned, scalable       | Production-grade pipelines        |\n",
    "\n",
    "> üìå Delta Tables support ACID transactions, schema evolution, and time travel.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Is a Model?\n",
    "\n",
    "A **model** in Unity Catalog is a registered machine learning asset. It includes:\n",
    "\n",
    "- Model files (e.g., `.pkl`, `.onnx`, `.pt`)\n",
    "- Metadata (version, creator, tags)\n",
    "- Permissions and lineage\n",
    "\n",
    "Models are stored inside schemas and can be versioned, served, and shared.\n",
    "\n",
    "‚úÖ Use for:\n",
    "- ML model governance\n",
    "- Reproducibility and auditability\n",
    "- Serving models via Databricks Model Serving\n",
    "\n",
    "‚ö†Ô∏è Avoid storing models outside Unity Catalog if you need traceability or sharing.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è How to Create These Assets\n",
    "\n",
    "### üîπ Create a Volume\n",
    "\n",
    "```sql\n",
    "CREATE VOLUME my_volume\n",
    "COMMENT 'Volume for storing raw images'\n",
    "```\n",
    "\n",
    "- For managed: no location needed.\n",
    "- For external: add `LOCATION 's3://my-bucket/path/'`\n",
    "\n",
    "### üîπ Create a Table\n",
    "\n",
    "```sql\n",
    "CREATE TABLE my_table (\n",
    "  id INT,\n",
    "  name STRING\n",
    ")\n",
    "USING DELTA\n",
    "```\n",
    "\n",
    "- You can also use `USING CSV`, `PARQUET`, etc.\n",
    "- For external tables, add `LOCATION '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8673a049-a07c-4cce-ba8d-abee832f57c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format('csv').option('inferSchema',True).option('header',True).load('/Volumes/pyspark_practice/default/files/Practice/BigMart Sales.csv')\n",
    "df_csv.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004d4b5c-9b7f-4094-b689-0bc587887633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note csv an json files are imported from my local machine , it is stored in Volumes . similar to volumes we can import csv as tables  . if imported as tables you cannot use above code , becuase it will throw error . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05d3959-898a-4dd2-9bdf-cdf0e2887476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detect available catalogs\n",
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "print(catalogs)\n",
    "\n",
    "# Define your table name\n",
    "table_name = \"big_mart_sales\"\n",
    "schema_name = \"default\"\n",
    "catalog_name = \"pyspark_practice\"  # or \"myorganization\" if that's your top-level catalog\n",
    "\n",
    "# Try Unity Catalog format if available\n",
    "if catalog_name in catalogs:\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "else:\n",
    "    full_table_name = f\"{schema_name}.{table_name}\"\n",
    "\n",
    "# Try loading the table\n",
    "try:\n",
    "    df = spark.table(full_table_name)\n",
    "    print(f\"‚úÖ Loaded table: {full_table_name}\")\n",
    "    df.display()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load table: {full_table_name}\")\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_read_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
