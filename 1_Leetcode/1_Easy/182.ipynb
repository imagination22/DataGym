{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca4d48a-be2d-45dd-9463-08a597f2936c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ§  Leetcode 182 â€” Duplicate Emails (Databricks Edition)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Problem Statement\n",
    "\n",
    "### Table: Person\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| id          | int     |\n",
    "| email       | varchar |\n",
    "\n",
    "- `id` is the primary key.\n",
    "- Each row contains an email.\n",
    "- Emails will not contain uppercase letters.\n",
    "- The `email` field is guaranteed to be **not NULL**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "Write a query to report **all duplicate emails**.  \n",
    "Return the result table in any order.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Example\n",
    "\n",
    "### Input\n",
    "\n",
    "**Person Table**\n",
    "\n",
    "| id | email   |\n",
    "|----|---------|\n",
    "| 1  | a@b.com |\n",
    "| 2  | c@d.com |\n",
    "| 3  | a@b.com |\n",
    "\n",
    "### Output\n",
    "\n",
    "| Email   |\n",
    "|---------|\n",
    "| a@b.com |\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- `a@b.com` appears twice.\n",
    "- Only emails that appear **more than once** should be returned.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± PySpark DataFrame Creation\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "person_data = [\n",
    "    Row(id=1, email=\"a@b.com\"),\n",
    "    Row(id=2, email=\"c@d.com\"),\n",
    "    Row(id=3, email=\"a@b.com\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "person_df = spark.createDataFrame(person_data)\n",
    "\n",
    "# Register temp view\n",
    "person_df.createOrReplaceTempView(\"Person\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… SQL Solution\n",
    "\n",
    "```sql\n",
    "SELECT email AS Email\n",
    "FROM Person\n",
    "GROUP BY email\n",
    "HAVING COUNT(*) > 1;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª PySpark Solution\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "result_df = person_df.groupBy(\"email\") \\\n",
    "                     .agg(F.count(\"*\").alias(\"cnt\")) \\\n",
    "                     .filter(\"cnt > 1\") \\\n",
    "                     .select(F.col(\"email\").alias(\"Email\"))\n",
    "\n",
    "result_df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ *This notebook is part of DataGymâ€™s SQL-to-PySpark transition series. Want to build a reusable template for aggregation-based problems? Letâ€™s co-create it!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b817ce10-af6c-4a5b-b467-3185ab661d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create sample data\n",
    "data = [\n",
    "    (1, \"a@b.com\"),\n",
    "    (2, \"c@d.com\"),\n",
    "    (3, \"a@b.com\")\n",
    "]\n",
    "\n",
    "# Step 2: Define schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),\n",
    "    StructField(\"email\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Step 4: Create temporary view\n",
    "df.createOrReplaceTempView(\"Person\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddcb8aae-42b5-49de-af5c-14a4535e076f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"email\").agg(count(\"email\").alias(\"cnt\")).filter(\"cnt > 1\").selectExpr(\"email AS Email\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7041df-6822-4ead-a5b6-61b988a889d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "note : it is selectExpr not selectexpr , E is capital else will throw error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4196114c-c97b-4241-b78e-a86c0052afe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"email\") \\\n",
    "  .agg(count(\"email\").alias(\"cnt\")) \\\n",
    "  .filter(\"cnt > 1\") \\\n",
    "  .selectExpr(\"email AS Email\") \\\n",
    "  .display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a308783d-80f8-425f-bcbf-b679dfae06da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"email\").orderBy(\"id\")\n",
    "\n",
    "# Add row_number column\n",
    "df_with_rn = df.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "# Filter rows where rn > 1 (i.e., duplicates)\n",
    "duplicate_emails = df_with_rn.filter(\"rn > 1\").selectExpr(\"email AS Email\")\n",
    "\n",
    "# Display result\n",
    "duplicate_emails.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b25c5a7-0027-4384-8a8f-8f445ea41efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: SQL query to find duplicate emails\n",
    "duplicate_emails = spark.sql(\"\"\"\n",
    "    SELECT email AS Email\n",
    "    FROM Person\n",
    "    GROUP BY email\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "# Step 6: Show result\n",
    "duplicate_emails.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "182",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
