{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf43f27a-7bda-4a11-85bc-5b433f4416df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ”¥ PySpark SparkSession Initialization â€” Explained for Databricks Users\n",
    "\n",
    "## ðŸ§  What This Code Does\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.some.config.option\", \"value\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "This snippet manually creates a `SparkSession`, which is the **gateway to Spark's features** in PySpark. Here's a quick breakdown:\n",
    "\n",
    "- `SparkSession.builder`: Starts the builder pattern to configure the session.\n",
    "- `.appName(\"MyApp\")`: Sets the name of the Spark application (visible in Spark UI).\n",
    "- `.master(\"local[*]\")`: Runs Spark locally using all available cores. This is used **outside clusters**, like on your laptop.\n",
    "- `.config(...)`: Adds custom Spark configuration. Replace `\"spark.some.config.option\"` with actual config keys.\n",
    "- `.getOrCreate()`: Returns an existing SparkSession or creates a new one.\n",
    "\n",
    "> âœ… This is **essential** when running PySpark scripts in standalone environments (e.g., VS Code, terminal, or Jupyter).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Why This Is *Not Required* in Databricks\n",
    "\n",
    "Databricks notebooks run inside a **fully managed Spark cluster**, which automatically provisions a `SparkSession` named `spark`.\n",
    "\n",
    "### What Databricks Already Handles:\n",
    "- âœ… SparkSession creation\n",
    "- âœ… Cluster resource management\n",
    "- âœ… Application naming and logging\n",
    "- âœ… Context-aware configuration\n",
    "\n",
    "### What You Donâ€™t Need to Do:\n",
    "- âŒ Call `.getOrCreate()` â€” it's already done.\n",
    "- âŒ Set `.master(\"local[*]\")` â€” Databricks uses cluster mode.\n",
    "- âŒ Manually configure basic session settings â€” many are managed by the platform.\n",
    "\n",
    "> ðŸŽ¬ Think of it like walking into a cinema with your own projectorâ€”redundant, but not disruptive.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ What Happens If You Run This in Databricks\n",
    "\n",
    "If you paste this code into a Databricks notebook:\n",
    "\n",
    "- âœ… It **wonâ€™t throw an error**.\n",
    "- ðŸ” It will **reuse or override** the existing SparkSession.\n",
    "- âš ï¸ `.master(\"local[*]\")` will be **ignored** or overridden by cluster settings.\n",
    "- ðŸ§© `.config(...)` may not apply if it conflicts with Databricks-managed configs.\n",
    "\n",
    "> ðŸ§¼ Best practice: Avoid redefining `spark` unless absolutely necessary.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Summary Comparison\n",
    "\n",
    "| Environment     | SparkSession Needed? | `.master(...)` Valid? | Default `spark` Available? |\n",
    "|----------------|----------------------|------------------------|-----------------------------|\n",
    "| PySpark Script | âœ… Yes               | âœ… Yes                 | âŒ No                       |\n",
    "| Databricks     | âŒ Not Required      | âŒ Ignored             | âœ… Yes                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Pro Tip for Reusability\n",
    "\n",
    "If you're writing notebooks that may run both inside and outside Databricks, use a conditional check:\n",
    "\n",
    "```python\n",
    "# Only needed when running outside Databricks\n",
    "if \"spark\" not in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MyApp\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ *This explanation is part of DataGymâ€™s onboarding series for PySpark learners. For more annotated examples and reusable notebook templates, check out the repository structure and contribution guide.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbee5359-b5f2-4aa9-aba1-3e1100bd0e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bcc866-36f6-4125-8659-24e293e2d4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "Select current_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d155856-88c4-42f8-9e54-aa01b47dc618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ§  Catalog vs Unity Catalog vs Hive Metastore â€” Explained for Databricks Users\n",
    "\n",
    "Letâ€™s unpack this like a layered ETL pipelineâ€”starting with the basics, then diving into architecture, governance, and production strategy. Here's a comprehensive breakdown of **Catalog vs Unity Catalog vs Hive Metastore**, including what Databricks offers in its free edition and how it all connects.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ What Is a Catalog?\n",
    "\n",
    "In Spark and Databricks, a **catalog** is a top-level container that organizes **schemas (databases)** and **tables**. Think of it as a folder system:\n",
    "\n",
    "```\n",
    "catalog.schema.table\n",
    "```\n",
    "\n",
    "- **Catalog**: Logical grouping (e.g., `main`, `hive_metastore`, `my_catalog`)\n",
    "- **Schema**: Like a database (e.g., `sales`, `marketing`)\n",
    "- **Table**: Actual data (e.g., `transactions`, `customers`)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ What Is Hive Metastore?\n",
    "\n",
    "The **Hive Metastore** is the traditional metadata store used in Hadoop and Spark ecosystems. It stores:\n",
    "\n",
    "- Table definitions\n",
    "- Schema info\n",
    "- Data locations (e.g., file paths in HDFS or S3)\n",
    "\n",
    "### ðŸ”— How Itâ€™s Related:\n",
    "- Spark uses Hive Metastore to resolve table names and schemas.\n",
    "- In Databricks, Hive Metastore is the **default catalog** called `hive_metastore`.\n",
    "\n",
    "> ðŸ“Œ In Databricks, you can query like:  \n",
    "> `SELECT * FROM hive_metastore.sales.transactions`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§­ What Is Unity Catalog?\n",
    "\n",
    "**Unity Catalog** is Databricksâ€™ modern, cloud-native metadata and governance layer. It replaces Hive Metastore with:\n",
    "\n",
    "- âœ… Fine-grained access control (column-level, row-level)\n",
    "- âœ… Cross-workspace governance\n",
    "- âœ… Built-in auditing and lineage tracking\n",
    "- âœ… Multi-cloud support (Azure, AWS, GCP)\n",
    "- âœ… REST APIs and integration with identity providers (like Entra ID)\n",
    "\n",
    "> ðŸ§  Unity Catalog introduces a **three-level namespace**:  \n",
    "> `catalog.schema.table` â€” making it easier to manage data across teams and environments.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª What Is V1 Catalog (aka Hive Metastore in Databricks)?\n",
    "\n",
    "- The **V1 Catalog** refers to the legacy Hive Metastore implementation in Databricks.\n",
    "- Itâ€™s **workspace-scoped** (each workspace has its own metastore).\n",
    "- Limited to **basic table-level permissions**.\n",
    "- No built-in lineage, audit logs, or cross-workspace sharing.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ†“ Databricks Free Edition (Community Edition)\n",
    "\n",
    "| Feature              | Available in Free Edition | Notes |\n",
    "|----------------------|---------------------------|-------|\n",
    "| Hive Metastore (V1)  | âœ… Yes                    | Default catalog (`hive_metastore`) |\n",
    "| Unity Catalog        | âŒ No                     | Requires premium tier or enterprise workspace |\n",
    "| Custom Catalogs      | âŒ No                     | Only `hive_metastore` is available |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ­ Which Should Be Used in Production?\n",
    "\n",
    "| Feature              | Hive Metastore (V1) | Unity Catalog |\n",
    "|----------------------|---------------------|----------------|\n",
    "| Governance           | âŒ Basic            | âœ… Advanced (column-level, row-level) |\n",
    "| Auditing             | âŒ Manual           | âœ… Built-in |\n",
    "| Lineage Tracking     | âŒ External tools   | âœ… Native |\n",
    "| Multi-workspace      | âŒ No               | âœ… Yes |\n",
    "| Cloud Integration    | âš ï¸ Limited         | âœ… Native |\n",
    "| Security             | âŒ Workspace-local | âœ… Identity-based |\n",
    "| Scalability          | âš ï¸ Bottlenecks     | âœ… Petabyte-scale |\n",
    "| Recommended for Prod | âŒ No               | âœ… Yes |\n",
    "\n",
    "> ðŸ§© Unity Catalog is the clear choice for production environments, especially when data governance, compliance, and collaboration are key.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¬ Similarities & Differences\n",
    "\n",
    "| Aspect               | Hive Metastore (V1)         | Unity Catalog                   |\n",
    "|----------------------|-----------------------------|----------------------------------|\n",
    "| Metadata Storage     | RDBMS (MySQL, Postgres)     | Cloud-native, managed by Databricks |\n",
    "| Namespace            | `database.table`            | `catalog.schema.table`          |\n",
    "| Access Control       | Table-level (basic)         | Fine-grained (column, row)      |\n",
    "| Integration          | Spark, Hive                 | Spark, Delta Lake, MLflow       |\n",
    "| Governance           | External tools (e.g., Ranger) | Built-in                        |\n",
    "| Multitenancy         | Single workspace            | Cross-workspace                 |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Migration Strategy\n",
    "\n",
    "If you're using Hive Metastore and want to move to Unity Catalog:\n",
    "\n",
    "1. **Upgrade tables** to Unity Catalog.\n",
    "2. **Federate Hive Metastore** into Unity Catalog as a foreign catalog (gradual migration).\n",
    "3. **Disable direct access** to Hive Metastore once migrated.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§µ TL;DR Summary\n",
    "\n",
    "- **Hive Metastore (V1)**: Legacy, workspace-scoped, basic governance. Default in free edition.\n",
    "- **Unity Catalog**: Modern, secure, scalable, cross-workspace. Recommended for production.\n",
    "- **Catalogs**: Logical containers for organizing schemas and tables. Unity Catalog supports multiple.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ *This explanation is part of DataGymâ€™s onboarding series for PySpark learners. For more annotated examples and reusable notebook templates, check out the repository structure and contribution guide.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e72766b-166e-4067-9a89-0f4ca6d5250d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ§  Databricks Catalog Concepts â€” Tables, Volumes, Models, Delta Shares & More\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ†“ What Does Databricks Free Edition Provide?\n",
    "\n",
    "| Feature              | Available in Free Edition | Notes |\n",
    "|----------------------|---------------------------|-------|\n",
    "| Hive Metastore (V1)  | âœ… Yes                    | Default catalog: `hive_metastore` |\n",
    "| Unity Catalog        | âŒ No                     | Requires premium/enterprise tier |\n",
    "| Delta Sharing        | âŒ No                     | Only available with Unity Catalog |\n",
    "| Volumes & Models     | âŒ No                     | Unity Catalog features only |\n",
    "\n",
    "> ðŸ“Œ In Free Edition, you're limited to the legacy `hive_metastore` catalog and basic table-level access control.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Tables vs Volumes vs Models in Unity Catalog\n",
    "\n",
    "Unity Catalog introduces **data and AI asset types** that live inside catalogs and schemas:\n",
    "\n",
    "### ðŸ§® Tables\n",
    "- Structured tabular data (rows & columns)\n",
    "- Can be **managed** (Databricks stores the files) or **external** (you manage the files)\n",
    "- Types:\n",
    "  - **Delta Tables**: Transactional, versioned, scalable\n",
    "  - **Parquet/CSV Tables**: Non-transactional, static\n",
    "- âœ… Use for: SQL queries, BI dashboards, ML training\n",
    "- âš ï¸ Avoid: CSV tables for productionâ€”no ACID guarantees\n",
    "\n",
    "### ðŸ“¦ Volumes\n",
    "- Unstructured or semi-structured file storage (like a folder)\n",
    "- Store images, PDFs, logs, JSON, etc.\n",
    "- Access via `dbutils.fs` or Spark APIs\n",
    "- âœ… Use for: ML datasets, raw ingestion, file-based workflows\n",
    "- âš ï¸ Avoid: Using volumes for structured tabular dataâ€”use tables instead\n",
    "\n",
    "### ðŸ¤– Models\n",
    "- Registered ML models (e.g., sklearn, XGBoost, PyTorch)\n",
    "- Stored with metadata, versioning, and permissions\n",
    "- Can be shared via Delta Sharing\n",
    "- âœ… Use for: Model serving, governance, reproducibility\n",
    "- âš ï¸ Avoid: Storing models outside Unity Catalog if you need auditability\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¢ My Organization vs ðŸŒ Delta Shares\n",
    "\n",
    "### ðŸ¢ My Organization\n",
    "- Refers to **data assets accessible within your Databricks account**\n",
    "- Includes all catalogs, schemas, tables, volumes, models you own\n",
    "- Governed by Unity Catalog (if enabled)\n",
    "\n",
    "### ðŸŒ Delta Shares\n",
    "- Mechanism to **share data across organizations**\n",
    "- Uses Unity Catalog to define **shares**, **recipients**, and **providers**\n",
    "- Supports sharing:\n",
    "  - Tables\n",
    "  - Views\n",
    "  - Volumes\n",
    "  - Models\n",
    "  - Notebooks\n",
    "\n",
    "> ðŸ“Œ Delta Shares are **not available in Free Edition**. They require Unity Catalog and a premium workspace.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š CSV Table vs Delta Table\n",
    "\n",
    "| Feature            | CSV Table (via `CREATE TABLE`) | Delta Table (via `CREATE TABLE USING DELTA`) |\n",
    "|--------------------|-------------------------------|----------------------------------------------|\n",
    "| Format             | CSV                           | Delta (Parquet + transaction log)            |\n",
    "| ACID Transactions  | âŒ No                          | âœ… Yes                                        |\n",
    "| Schema Evolution   | âŒ Manual                     | âœ… Automatic                                  |\n",
    "| Time Travel        | âŒ No                          | âœ… Yes                                        |\n",
    "| Performance        | âš ï¸ Slower                     | âœ… Optimized for big data                     |\n",
    "| Recommended for    | Prototyping, small datasets    | Production, scalable pipelines               |\n",
    "\n",
    "> âœ… Always prefer **Delta Tables** for production workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± Is Everything Inside \"My Organization\" a Database?\n",
    "\n",
    "Not quite. Here's how Databricks organizes data:\n",
    "\n",
    "### ðŸ”¹ Catalog\n",
    "- Top-level container (e.g., `main`, `hive_metastore`, `my_catalog`)\n",
    "- Unity Catalog supports multiple catalogs\n",
    "\n",
    "### ðŸ”¹ Schema (aka Database)\n",
    "- Logical grouping of tables, views, volumes, models\n",
    "- Examples: `default`, `sales`, `ml_models`\n",
    "\n",
    "### ðŸ”¹ Table / Volume / Model\n",
    "- Actual data or AI asset\n",
    "\n",
    "### ðŸ”¹ Special Schemas\n",
    "- `default`: Default schema inside a catalog\n",
    "- `information_schema`: System schema with metadata tables (e.g., `tables`, `columns`, `views`)\n",
    "  - âœ… Use for: Auditing, introspection, governance\n",
    "\n",
    "> ðŸ“Œ So yes, `default` and `information_schema` are schemas (not catalogs), and they live inside a catalog like `main` or `hive_metastore`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§µ TL;DR Summary\n",
    "\n",
    "- **Free Edition** gives you `hive_metastore` with basic tables.\n",
    "- **Unity Catalog** adds volumes, models, Delta Sharing, and governance.\n",
    "- **Tables** = structured data, **Volumes** = files, **Models** = ML assets.\n",
    "- **Delta Tables** > CSV Tables for production.\n",
    "- **My Organization** = internal assets; **Delta Shares** = external sharing.\n",
    "- `default` and `information_schema` are schemas inside a catalog.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ *This guide is part of DataGymâ€™s onboarding series. For visual cheat sheets or migration playbooks, letâ€™s co-design something stunning!*\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8306450320142204,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_spark_in_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
