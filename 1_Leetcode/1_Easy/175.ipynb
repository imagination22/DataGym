{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa309a3-64fe-4105-9c64-8f7e0a40fb64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ§  Leetcode 175 â€” Combine Two Tables (Databricks Edition)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Problem Statement\n",
    "\n",
    "You are given two tables:\n",
    "\n",
    "### Table: Person\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| personId    | int     |\n",
    "| lastName    | varchar |\n",
    "| firstName   | varchar |\n",
    "\n",
    "- `personId` is the primary key.\n",
    "- This table contains information about the ID of some persons and their first and last names.\n",
    "\n",
    "---\n",
    "\n",
    "### Table: Address\n",
    "\n",
    "| Column Name | Type    |\n",
    "|-------------|---------|\n",
    "| addressId   | int     |\n",
    "| personId    | int     |\n",
    "| city        | varchar |\n",
    "| state       | varchar |\n",
    "\n",
    "- `addressId` is the primary key.\n",
    "- Each row contains information about the city and state of one person with ID = `personId`.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "Write a query to report the `firstName`, `lastName`, `city`, and `state` of each person in the `Person` table.  \n",
    "If the address of a `personId` is not present in the `Address` table, report `null` instead.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Example\n",
    "\n",
    "### Input\n",
    "\n",
    "**Person Table**\n",
    "\n",
    "| personId | lastName | firstName |\n",
    "|----------|----------|-----------|\n",
    "| 1        | Wang     | Allen     |\n",
    "| 2        | Alice    | Bob       |\n",
    "\n",
    "**Address Table**\n",
    "\n",
    "| addressId | personId | city          | state      |\n",
    "|-----------|----------|---------------|------------|\n",
    "| 1         | 2        | New York City | New York   |\n",
    "| 2         | 3        | Leetcode      | California |\n",
    "\n",
    "### Output\n",
    "\n",
    "| firstName | lastName | city          | state    |\n",
    "|-----------|----------|---------------|----------|\n",
    "| Allen     | Wang     | null          | null     |\n",
    "| Bob       | Alice    | New York City | New York |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784d12de-add6-46b4-be49-335fcaa5840d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "person_data = [\n",
    "    Row(personId=1, lastName=\"Wang\", firstName=\"Allen\"),\n",
    "    Row(personId=2, lastName=\"Alice\", firstName=\"Bob\")\n",
    "]\n",
    "\n",
    "address_data = [\n",
    "    Row(addressId=1, personId=2, city=\"New York City\", state=\"New York\"),\n",
    "    Row(addressId=2, personId=3, city=\"Leetcode\", state=\"California\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "person_df = spark.createDataFrame(person_data)\n",
    "address_df = spark.createDataFrame(address_data)\n",
    "\n",
    "# Register temp views\n",
    "person_df.createOrReplaceTempView(\"Person\")\n",
    "address_df.createOrReplaceTempView(\"Address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f97d77b-c7f1-404e-b9ac-ce0bce888459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    p.firstName,\n",
    "    p.lastName,\n",
    "    a.city,\n",
    "    a.state\n",
    "FROM Person p\n",
    "LEFT JOIN Address a\n",
    "ON p.personId = a.personId;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96d8d39-bca5-4b55-a643-fc5bcf638ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "result_df = person_df.join(\n",
    "    address_df,\n",
    "    on=\"personId\",\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    \"firstName\", \"lastName\", \"city\", \"state\"\n",
    ")\n",
    "\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e3a7b8-0514-4f60-879f-b8f4397334f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ§  PySpark Join Disambiguation â€” Handling Overlapping Column Names\n",
    "\n",
    "When joining two DataFrames that share column names (other than the join key), PySpark will raise ambiguity unless you explicitly handle the overlaps. Here's how to resolve it cleanly and confidently.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Problem\n",
    "\n",
    "If both DataFrames have a column named `\"id\"` and `\"name\"`, and you join on `\"id\"`, PySpark wonâ€™t know which `\"name\"` to use unless you disambiguate.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Solutions\n",
    "\n",
    "### 1. ðŸ”§ Use Aliases Before Join\n",
    "\n",
    "Rename columns using `.alias()` or `selectExpr()` to avoid conflicts:\n",
    "\n",
    "```python\n",
    "df1 = df1.selectExpr(\"id\", \"name as name_df1\")\n",
    "df2 = df2.selectExpr(\"id\", \"name as name_df2\")\n",
    "\n",
    "joined_df = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ðŸ§¹ Drop Duplicate Columns After Join\n",
    "\n",
    "If you only need one version of the overlapping column:\n",
    "\n",
    "```python\n",
    "joined_df = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "joined_df = joined_df.drop(df2[\"name\"])  # Keep df1's \"name\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ðŸŽ¯ Select Specific Columns Post-Join\n",
    "\n",
    "Explicitly choose which columns to keep:\n",
    "\n",
    "```python\n",
    "joined_df = df1.join(df2, on=\"id\", how=\"inner\") \\\n",
    "               .select(df1[\"id\"], df1[\"name\"], df2[\"age\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ðŸ§  Use Aliases for DataFrames\n",
    "\n",
    "This is handy for SQL-style joins and readable column references:\n",
    "\n",
    "```python\n",
    "df1_alias = df1.alias(\"a\")\n",
    "df2_alias = df2.alias(\"b\")\n",
    "\n",
    "joined_df = df1_alias.join(df2_alias, df1_alias[\"id\"] == df2_alias[\"id\"]) \\\n",
    "                     .select(\"a.id\", \"a.name\", \"b.name\", \"b.age\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Bonus Tip for Debugging\n",
    "\n",
    "Use `.printSchema()` after join to inspect column lineage and catch any surprises:\n",
    "\n",
    "```python\n",
    "joined_df.printSchema()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ *Want a reusable function or cheat sheet for this? I can help you build a PySpark join disambiguation module tailored for DataGym contributorsâ€”complete with examples, edge cases, and visual metaphors!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c161941f-de11-4e6c-829c-d62b5ef8ca34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6712418290803926,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "175",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
