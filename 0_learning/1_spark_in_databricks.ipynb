{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf43f27a-7bda-4a11-85bc-5b433f4416df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔥 PySpark SparkSession Initialization — Explained for Databricks Users\n",
    "\n",
    "## 🧠 What This Code Does\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.some.config.option\", \"value\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "This snippet manually creates a `SparkSession`, which is the **gateway to Spark's features** in PySpark. Here's a quick breakdown:\n",
    "\n",
    "- `SparkSession.builder`: Starts the builder pattern to configure the session.\n",
    "- `.appName(\"MyApp\")`: Sets the name of the Spark application (visible in Spark UI).\n",
    "- `.master(\"local[*]\")`: Runs Spark locally using all available cores. This is used **outside clusters**, like on your laptop.\n",
    "- `.config(...)`: Adds custom Spark configuration. Replace `\"spark.some.config.option\"` with actual config keys.\n",
    "- `.getOrCreate()`: Returns an existing SparkSession or creates a new one.\n",
    "\n",
    "> ✅ This is **essential** when running PySpark scripts in standalone environments (e.g., VS Code, terminal, or Jupyter).\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Why This Is *Not Required* in Databricks\n",
    "\n",
    "Databricks notebooks run inside a **fully managed Spark cluster**, which automatically provisions a `SparkSession` named `spark`.\n",
    "\n",
    "### What Databricks Already Handles:\n",
    "- ✅ SparkSession creation\n",
    "- ✅ Cluster resource management\n",
    "- ✅ Application naming and logging\n",
    "- ✅ Context-aware configuration\n",
    "\n",
    "### What You Don’t Need to Do:\n",
    "- ❌ Call `.getOrCreate()` — it's already done.\n",
    "- ❌ Set `.master(\"local[*]\")` — Databricks uses cluster mode.\n",
    "- ❌ Manually configure basic session settings — many are managed by the platform.\n",
    "\n",
    "> 🎬 Think of it like walking into a cinema with your own projector—redundant, but not disruptive.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ What Happens If You Run This in Databricks\n",
    "\n",
    "If you paste this code into a Databricks notebook:\n",
    "\n",
    "- ✅ It **won’t throw an error**.\n",
    "- 🔁 It will **reuse or override** the existing SparkSession.\n",
    "- ⚠️ `.master(\"local[*]\")` will be **ignored** or overridden by cluster settings.\n",
    "- 🧩 `.config(...)` may not apply if it conflicts with Databricks-managed configs.\n",
    "\n",
    "> 🧼 Best practice: Avoid redefining `spark` unless absolutely necessary.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Summary Comparison\n",
    "\n",
    "| Environment     | SparkSession Needed? | `.master(...)` Valid? | Default `spark` Available? |\n",
    "|----------------|----------------------|------------------------|-----------------------------|\n",
    "| PySpark Script | ✅ Yes               | ✅ Yes                 | ❌ No                       |\n",
    "| Databricks     | ❌ Not Required      | ❌ Ignored             | ✅ Yes                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Pro Tip for Reusability\n",
    "\n",
    "If you're writing notebooks that may run both inside and outside Databricks, use a conditional check:\n",
    "\n",
    "```python\n",
    "# Only needed when running outside Databricks\n",
    "if \"spark\" not in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MyApp\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "📘 *This explanation is part of DataGym’s onboarding series for PySpark learners. For more annotated examples and reusable notebook templates, check out the repository structure and contribution guide.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbee5359-b5f2-4aa9-aba1-3e1100bd0e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bcc866-36f6-4125-8659-24e293e2d4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "Select current_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d155856-88c4-42f8-9e54-aa01b47dc618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🧠 Catalog vs Unity Catalog vs Hive Metastore — Explained for Databricks Users\n",
    "\n",
    "Let’s unpack this like a layered ETL pipeline—starting with the basics, then diving into architecture, governance, and production strategy. Here's a comprehensive breakdown of **Catalog vs Unity Catalog vs Hive Metastore**, including what Databricks offers in its free edition and how it all connects.\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 What Is a Catalog?\n",
    "\n",
    "In Spark and Databricks, a **catalog** is a top-level container that organizes **schemas (databases)** and **tables**. Think of it as a folder system:\n",
    "\n",
    "```\n",
    "catalog.schema.table\n",
    "```\n",
    "\n",
    "- **Catalog**: Logical grouping (e.g., `main`, `hive_metastore`, `my_catalog`)\n",
    "- **Schema**: Like a database (e.g., `sales`, `marketing`)\n",
    "- **Table**: Actual data (e.g., `transactions`, `customers`)\n",
    "\n",
    "---\n",
    "\n",
    "## 🐝 What Is Hive Metastore?\n",
    "\n",
    "The **Hive Metastore** is the traditional metadata store used in Hadoop and Spark ecosystems. It stores:\n",
    "\n",
    "- Table definitions\n",
    "- Schema info\n",
    "- Data locations (e.g., file paths in HDFS or S3)\n",
    "\n",
    "### 🔗 How It’s Related:\n",
    "- Spark uses Hive Metastore to resolve table names and schemas.\n",
    "- In Databricks, Hive Metastore is the **default catalog** called `hive_metastore`.\n",
    "\n",
    "> 📌 In Databricks, you can query like:  \n",
    "> `SELECT * FROM hive_metastore.sales.transactions`\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 What Is Unity Catalog?\n",
    "\n",
    "**Unity Catalog** is Databricks’ modern, cloud-native metadata and governance layer. It replaces Hive Metastore with:\n",
    "\n",
    "- ✅ Fine-grained access control (column-level, row-level)\n",
    "- ✅ Cross-workspace governance\n",
    "- ✅ Built-in auditing and lineage tracking\n",
    "- ✅ Multi-cloud support (Azure, AWS, GCP)\n",
    "- ✅ REST APIs and integration with identity providers (like Entra ID)\n",
    "\n",
    "> 🧠 Unity Catalog introduces a **three-level namespace**:  \n",
    "> `catalog.schema.table` — making it easier to manage data across teams and environments.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 What Is V1 Catalog (aka Hive Metastore in Databricks)?\n",
    "\n",
    "- The **V1 Catalog** refers to the legacy Hive Metastore implementation in Databricks.\n",
    "- It’s **workspace-scoped** (each workspace has its own metastore).\n",
    "- Limited to **basic table-level permissions**.\n",
    "- No built-in lineage, audit logs, or cross-workspace sharing.\n",
    "\n",
    "---\n",
    "\n",
    "## 🆓 Databricks Free Edition (Community Edition)\n",
    "\n",
    "| Feature              | Available in Free Edition | Notes |\n",
    "|----------------------|---------------------------|-------|\n",
    "| Hive Metastore (V1)  | ✅ Yes                    | Default catalog (`hive_metastore`) |\n",
    "| Unity Catalog        | ❌ No                     | Requires premium tier or enterprise workspace |\n",
    "| Custom Catalogs      | ❌ No                     | Only `hive_metastore` is available |\n",
    "\n",
    "---\n",
    "\n",
    "## 🏭 Which Should Be Used in Production?\n",
    "\n",
    "| Feature              | Hive Metastore (V1) | Unity Catalog |\n",
    "|----------------------|---------------------|----------------|\n",
    "| Governance           | ❌ Basic            | ✅ Advanced (column-level, row-level) |\n",
    "| Auditing             | ❌ Manual           | ✅ Built-in |\n",
    "| Lineage Tracking     | ❌ External tools   | ✅ Native |\n",
    "| Multi-workspace      | ❌ No               | ✅ Yes |\n",
    "| Cloud Integration    | ⚠️ Limited         | ✅ Native |\n",
    "| Security             | ❌ Workspace-local | ✅ Identity-based |\n",
    "| Scalability          | ⚠️ Bottlenecks     | ✅ Petabyte-scale |\n",
    "| Recommended for Prod | ❌ No               | ✅ Yes |\n",
    "\n",
    "> 🧩 Unity Catalog is the clear choice for production environments, especially when data governance, compliance, and collaboration are key.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 Similarities & Differences\n",
    "\n",
    "| Aspect               | Hive Metastore (V1)         | Unity Catalog                   |\n",
    "|----------------------|-----------------------------|----------------------------------|\n",
    "| Metadata Storage     | RDBMS (MySQL, Postgres)     | Cloud-native, managed by Databricks |\n",
    "| Namespace            | `database.table`            | `catalog.schema.table`          |\n",
    "| Access Control       | Table-level (basic)         | Fine-grained (column, row)      |\n",
    "| Integration          | Spark, Hive                 | Spark, Delta Lake, MLflow       |\n",
    "| Governance           | External tools (e.g., Ranger) | Built-in                        |\n",
    "| Multitenancy         | Single workspace            | Cross-workspace                 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Migration Strategy\n",
    "\n",
    "If you're using Hive Metastore and want to move to Unity Catalog:\n",
    "\n",
    "1. **Upgrade tables** to Unity Catalog.\n",
    "2. **Federate Hive Metastore** into Unity Catalog as a foreign catalog (gradual migration).\n",
    "3. **Disable direct access** to Hive Metastore once migrated.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧵 TL;DR Summary\n",
    "\n",
    "- **Hive Metastore (V1)**: Legacy, workspace-scoped, basic governance. Default in free edition.\n",
    "- **Unity Catalog**: Modern, secure, scalable, cross-workspace. Recommended for production.\n",
    "- **Catalogs**: Logical containers for organizing schemas and tables. Unity Catalog supports multiple.\n",
    "\n",
    "---\n",
    "\n",
    "📘 *This explanation is part of DataGym’s onboarding series for PySpark learners. For more annotated examples and reusable notebook templates, check out the repository structure and contribution guide.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e72766b-166e-4067-9a89-0f4ca6d5250d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🧠 Databricks Catalog Concepts — Tables, Volumes, Models, Delta Shares & More\n",
    "\n",
    "---\n",
    "\n",
    "## 🆓 What Does Databricks Free Edition Provide?\n",
    "\n",
    "| Feature              | Available in Free Edition | Notes |\n",
    "|----------------------|---------------------------|-------|\n",
    "| Hive Metastore (V1)  | ✅ Yes                    | Default catalog: `hive_metastore` |\n",
    "| Unity Catalog        | ❌ No                     | Requires premium/enterprise tier |\n",
    "| Delta Sharing        | ❌ No                     | Only available with Unity Catalog |\n",
    "| Volumes & Models     | ❌ No                     | Unity Catalog features only |\n",
    "\n",
    "> 📌 In Free Edition, you're limited to the legacy `hive_metastore` catalog and basic table-level access control.\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Tables vs Volumes vs Models in Unity Catalog\n",
    "\n",
    "Unity Catalog introduces **data and AI asset types** that live inside catalogs and schemas:\n",
    "\n",
    "### 🧮 Tables\n",
    "- Structured tabular data (rows & columns)\n",
    "- Can be **managed** (Databricks stores the files) or **external** (you manage the files)\n",
    "- Types:\n",
    "  - **Delta Tables**: Transactional, versioned, scalable\n",
    "  - **Parquet/CSV Tables**: Non-transactional, static\n",
    "- ✅ Use for: SQL queries, BI dashboards, ML training\n",
    "- ⚠️ Avoid: CSV tables for production—no ACID guarantees\n",
    "\n",
    "### 📦 Volumes\n",
    "- Unstructured or semi-structured file storage (like a folder)\n",
    "- Store images, PDFs, logs, JSON, etc.\n",
    "- Access via `dbutils.fs` or Spark APIs\n",
    "- ✅ Use for: ML datasets, raw ingestion, file-based workflows\n",
    "- ⚠️ Avoid: Using volumes for structured tabular data—use tables instead\n",
    "\n",
    "### 🤖 Models\n",
    "- Registered ML models (e.g., sklearn, XGBoost, PyTorch)\n",
    "- Stored with metadata, versioning, and permissions\n",
    "- Can be shared via Delta Sharing\n",
    "- ✅ Use for: Model serving, governance, reproducibility\n",
    "- ⚠️ Avoid: Storing models outside Unity Catalog if you need auditability\n",
    "\n",
    "---\n",
    "\n",
    "## 🏢 My Organization vs 🌐 Delta Shares\n",
    "\n",
    "### 🏢 My Organization\n",
    "- Refers to **data assets accessible within your Databricks account**\n",
    "- Includes all catalogs, schemas, tables, volumes, models you own\n",
    "- Governed by Unity Catalog (if enabled)\n",
    "\n",
    "### 🌐 Delta Shares\n",
    "- Mechanism to **share data across organizations**\n",
    "- Uses Unity Catalog to define **shares**, **recipients**, and **providers**\n",
    "- Supports sharing:\n",
    "  - Tables\n",
    "  - Views\n",
    "  - Volumes\n",
    "  - Models\n",
    "  - Notebooks\n",
    "\n",
    "> 📌 Delta Shares are **not available in Free Edition**. They require Unity Catalog and a premium workspace.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 CSV Table vs Delta Table\n",
    "\n",
    "| Feature            | CSV Table (via `CREATE TABLE`) | Delta Table (via `CREATE TABLE USING DELTA`) |\n",
    "|--------------------|-------------------------------|----------------------------------------------|\n",
    "| Format             | CSV                           | Delta (Parquet + transaction log)            |\n",
    "| ACID Transactions  | ❌ No                          | ✅ Yes                                        |\n",
    "| Schema Evolution   | ❌ Manual                     | ✅ Automatic                                  |\n",
    "| Time Travel        | ❌ No                          | ✅ Yes                                        |\n",
    "| Performance        | ⚠️ Slower                     | ✅ Optimized for big data                     |\n",
    "| Recommended for    | Prototyping, small datasets    | Production, scalable pipelines               |\n",
    "\n",
    "> ✅ Always prefer **Delta Tables** for production workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Is Everything Inside \"My Organization\" a Database?\n",
    "\n",
    "Not quite. Here's how Databricks organizes data:\n",
    "\n",
    "### 🔹 Catalog\n",
    "- Top-level container (e.g., `main`, `hive_metastore`, `my_catalog`)\n",
    "- Unity Catalog supports multiple catalogs\n",
    "\n",
    "### 🔹 Schema (aka Database)\n",
    "- Logical grouping of tables, views, volumes, models\n",
    "- Examples: `default`, `sales`, `ml_models`\n",
    "\n",
    "### 🔹 Table / Volume / Model\n",
    "- Actual data or AI asset\n",
    "\n",
    "### 🔹 Special Schemas\n",
    "- `default`: Default schema inside a catalog\n",
    "- `information_schema`: System schema with metadata tables (e.g., `tables`, `columns`, `views`)\n",
    "  - ✅ Use for: Auditing, introspection, governance\n",
    "\n",
    "> 📌 So yes, `default` and `information_schema` are schemas (not catalogs), and they live inside a catalog like `main` or `hive_metastore`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧵 TL;DR Summary\n",
    "\n",
    "- **Free Edition** gives you `hive_metastore` with basic tables.\n",
    "- **Unity Catalog** adds volumes, models, Delta Sharing, and governance.\n",
    "- **Tables** = structured data, **Volumes** = files, **Models** = ML assets.\n",
    "- **Delta Tables** > CSV Tables for production.\n",
    "- **My Organization** = internal assets; **Delta Shares** = external sharing.\n",
    "- `default` and `information_schema` are schemas inside a catalog.\n",
    "\n",
    "---\n",
    "\n",
    "📘 *This guide is part of DataGym’s onboarding series. For visual cheat sheets or migration playbooks, let’s co-design something stunning!*\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8306450320142204,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_spark_in_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
