{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea57011-3421-43b4-8eb6-89b5b7601a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.format('json').option('inferSchema', True)\\\n",
    "                    .option('header', True)\\\n",
    "                    .option('multiLine', False)\\\n",
    "                    .load('/Volumes/pyspark_practice/default/files/Practice/drivers.json')\n",
    "df_json.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ed86d7-cf3e-4b8b-9dda-d7eca0d0fd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß† Unity Catalog Assets ‚Äî Volumes, Tables, and Models Explained\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ What Is a Volume?\n",
    "\n",
    "A **volume** in Unity Catalog is a governed storage container for **non-tabular data**‚Äîthink files like images, logs, PDFs, JSON, or ML datasets. It‚Äôs designed to complement tables by handling unstructured and semi-structured data.\n",
    "\n",
    "### üîπ Is a Volume Created Inside a Schema?\n",
    "\n",
    "Yes! In Unity Catalog, volumes are **siblings** to tables, views, and models. They live inside a **schema**, which itself belongs to a **catalog**.\n",
    "\n",
    "> üìÅ Path format: `/Volumes/<catalog>/<schema>/<volume>/<path>/<file>`\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Types of Volumes\n",
    "\n",
    "### 1. **Managed Volume**\n",
    "- Created and fully governed by Unity Catalog.\n",
    "- No need to specify a location‚ÄîDatabricks handles storage.\n",
    "- File access is only through Unity Catalog paths.\n",
    "- Deleted volumes retain files for 7 days before cleanup.\n",
    "\n",
    "‚úÖ Use when:\n",
    "- You want simple governance.\n",
    "- You don‚Äôt need external access to the files.\n",
    "\n",
    "‚ö†Ô∏è Avoid when:\n",
    "- You need direct cloud URI access or external system integration.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **External Volume**\n",
    "- Points to a directory in your cloud storage (e.g., S3, ADLS).\n",
    "- You specify the location during creation.\n",
    "- Unity Catalog governs access, but external systems can still read/write directly.\n",
    "\n",
    "‚úÖ Use when:\n",
    "- You already have data in cloud storage.\n",
    "- You want to apply governance without migrating files.\n",
    "\n",
    "‚ö†Ô∏è Avoid when:\n",
    "- You want Unity Catalog to fully manage lifecycle and cleanup.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä What Is a Table?\n",
    "\n",
    "A **table** is a structured dataset with rows and columns. In Unity Catalog, tables are governed objects inside schemas.\n",
    "\n",
    "### üîπ Types of Tables\n",
    "\n",
    "| Type           | Description                              | Use Case                          |\n",
    "|----------------|------------------------------------------|-----------------------------------|\n",
    "| Managed Table  | Databricks manages both data & metadata  | Internal-only workflows           |\n",
    "| External Table | Data lives in external storage           | Shared or multi-tool environments |\n",
    "| Delta Table    | Transactional, versioned, scalable       | Production-grade pipelines        |\n",
    "\n",
    "> üìå Delta Tables support ACID transactions, schema evolution, and time travel.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Is a Model?\n",
    "\n",
    "A **model** in Unity Catalog is a registered machine learning asset. It includes:\n",
    "\n",
    "- Model files (e.g., `.pkl`, `.onnx`, `.pt`)\n",
    "- Metadata (version, creator, tags)\n",
    "- Permissions and lineage\n",
    "\n",
    "Models are stored inside schemas and can be versioned, served, and shared.\n",
    "\n",
    "‚úÖ Use for:\n",
    "- ML model governance\n",
    "- Reproducibility and auditability\n",
    "- Serving models via Databricks Model Serving\n",
    "\n",
    "‚ö†Ô∏è Avoid storing models outside Unity Catalog if you need traceability or sharing.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è How to Create These Assets\n",
    "\n",
    "### üîπ Create a Volume\n",
    "\n",
    "```sql\n",
    "CREATE VOLUME my_volume\n",
    "COMMENT 'Volume for storing raw images'\n",
    "```\n",
    "\n",
    "- For managed: no location needed.\n",
    "- For external: add `LOCATION 's3://my-bucket/path/'`\n",
    "\n",
    "### üîπ Create a Table\n",
    "\n",
    "```sql\n",
    "CREATE TABLE my_table (\n",
    "  id INT,\n",
    "  name STRING\n",
    ")\n",
    "USING DELTA\n",
    "```\n",
    "\n",
    "- You can also use `USING CSV`, `PARQUET`, etc.\n",
    "- For external tables, add `LOCATION '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8673a049-a07c-4cce-ba8d-abee832f57c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format('csv').option('inferSchema',True).option('header',True).load('/Volumes/pyspark_practice/default/files/Practice/BigMart Sales.csv')\n",
    "df_csv.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004d4b5c-9b7f-4094-b689-0bc587887633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note csv an json files are imported from my local machine , it is stored in Volumes . similar to volumes we can import csv as tables  . if imported as tables you cannot use above code , becuase it will throw error . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05d3959-898a-4dd2-9bdf-cdf0e2887476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detect available catalogs\n",
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "print(catalogs)\n",
    "\n",
    "# Define your table name\n",
    "table_name = \"big_mart_sales\"\n",
    "schema_name = \"default\"\n",
    "catalog_name = \"pyspark_practice\"  # or \"myorganization\" if that's your top-level catalog\n",
    "\n",
    "# Try Unity Catalog format if available\n",
    "if catalog_name in catalogs:\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "else:\n",
    "    full_table_name = f\"{schema_name}.{table_name}\"\n",
    "\n",
    "# Try loading the table\n",
    "try:\n",
    "    df = spark.table(full_table_name)\n",
    "    print(f\"‚úÖ Loaded table: {full_table_name}\")\n",
    "    df.display()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load table: {full_table_name}\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7603df1a-f21e-409c-8214-73fea8b1d96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß† PySpark Data Types ‚Äî DDL vs StructType Cheat Sheet\n",
    "\n",
    "Let‚Äôs break down PySpark data types into two intuitive categories: **DDL-style** and **Struct-based**. Think of it as SQL-like string schemas vs. Pythonic object schemas‚Äîeach with its own strengths depending on your use case.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. DDL-style (String-based schema)\n",
    "\n",
    "This is the SQL-inspired way to define schemas using strings. It‚Äôs compact and often used when reading data from external sources like CSV or JSON.\n",
    "\n",
    "### ‚úÖ Best for:\n",
    "- Quick schema definitions\n",
    "- Config-driven pipelines\n",
    "- Lightweight ingestion scripts\n",
    "\n",
    "### üìå Syntax\n",
    "\n",
    "```python\n",
    "ddl_schema = \"name STRING, age INT, salary DOUBLE\"\n",
    "```\n",
    "\n",
    "### üß¨ Supported Types\n",
    "\n",
    "| DDL Type     |\n",
    "|--------------|\n",
    "| STRING       |\n",
    "| INT          |\n",
    "| BIGINT       |\n",
    "| DOUBLE       |\n",
    "| BOOLEAN      |\n",
    "| DATE         |\n",
    "| TIMESTAMP    |\n",
    "| ARRAY<...>   |\n",
    "| MAP<...>     |\n",
    "| STRUCT<...>  |\n",
    "\n",
    "### üß™ Example with Nesting\n",
    "\n",
    "```python\n",
    "ddl_schema = \"user STRUCT<id: INT, name: STRING>, scores ARRAY<DOUBLE>\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ 2. Struct-based (Programmatic schema)\n",
    "\n",
    "This is the object-oriented way using `StructType` and `StructField`. It‚Äôs verbose but gives you full control‚Äîideal for dynamic schema generation and validation.\n",
    "\n",
    "### ‚úÖ Best for:\n",
    "- Complex pipelines\n",
    "- Dynamic schema manipulation\n",
    "- UDFs and typed transformations\n",
    "\n",
    "### üìå Syntax\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "struct_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "```\n",
    "\n",
    "### üß¨ Supported Types\n",
    "\n",
    "| Struct Type       |\n",
    "|-------------------|\n",
    "| StringType()      |\n",
    "| IntegerType()     |\n",
    "| LongType()        |\n",
    "| DoubleType()      |\n",
    "| BooleanType()     |\n",
    "| DateType()        |\n",
    "| TimestampType()   |\n",
    "| ArrayType(...)    |\n",
    "| MapType(...)      |\n",
    "| StructType([...]) |\n",
    "\n",
    "### üß™ Example with Nesting\n",
    "\n",
    "```python\n",
    "nested_schema = StructType([\n",
    "    StructField(\"user\", StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"scores\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Comparison Table\n",
    "\n",
    "| Feature               | DDL-style (String) | Struct-based (Object) |\n",
    "|------------------------|--------------------|------------------------|\n",
    "| Syntax Style           | SQL-like string     | Pythonic object        |\n",
    "| Verbosity              | Compact             | Verbose                |\n",
    "| Nesting Support        | ‚úÖ Yes              | ‚úÖ Yes                 |\n",
    "| Dynamic Generation     | ‚ö†Ô∏è Limited         | ‚úÖ Full control        |\n",
    "| Best For               | Ingestion, configs  | UDFs, transformations  |\n",
    "| Type Safety            | ‚ùå No               | ‚úÖ Yes                 |\n",
    "| Validation             | ‚ùå Manual           | ‚úÖ Built-in            |\n",
    "\n",
    "---\n",
    "\n",
    "üìò *Would you like a utility to convert DDL to StructType dynamically? Or maybe a visual cheat sheet for your DataGym repo that maps DDL types to Struct types with examples? I‚Äôd love to help you build that!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3a6c0e-a89c-4b6c-b062-a813bb661721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        \"Alice\", 29, 1001, 2500.75, True, date(1996, 5, 14), datetime(2025, 9, 19, 10, 30, 0),\n",
    "        [\"Python\", \"SQL\"],\n",
    "        {\"theme\": \"dark\", \"language\": \"en\"},\n",
    "        {\"city\": \"Mumbai\", \"zip\": \"400601\"}\n",
    "    ),\n",
    "    (\n",
    "        \"Bob\", 35, 1002, 1800.50, False, date(1989, 11, 2), datetime(2025, 9, 19, 11, 15, 0),\n",
    "        [\"Java\", \"Scala\"],\n",
    "        {\"theme\": \"light\", \"language\": \"fr\"},\n",
    "        {\"city\": \"Pune\", \"zip\": \"411001\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "struct_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"user_id\", LongType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"birthdate\", DateType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"preferences\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"zip\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "df_struct = spark.createDataFrame(data, schema=struct_schema)\n",
    "df_struct.show(truncate=False)\n",
    "df_struct.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1fc2be-437e-4e0a-bcf8-dbeec6c6a915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "data = [\n",
    "    (\n",
    "        \"Alice\",                      # STRING\n",
    "        29,                           # INT\n",
    "        1001,                         # BIGINT\n",
    "        2500.75,                      # DOUBLE\n",
    "        True,                         # BOOLEAN\n",
    "        date(1996, 5, 14),            # DATE\n",
    "        datetime(2025, 9, 19, 10, 30),# TIMESTAMP\n",
    "        [\"Python\", \"SQL\"],            # ARRAY<STRING>\n",
    "        {\"theme\": \"dark\", \"lang\": \"en\"}, # MAP<STRING, STRING>\n",
    "        {\"city\": \"Mumbai\", \"zip\": \"400601\"} # STRUCT<city, zip>\n",
    "    ),\n",
    "    (\n",
    "        \"Bob\",\n",
    "        35,\n",
    "        1002,\n",
    "        1800.50,\n",
    "        False,\n",
    "        date(1989, 11, 2),\n",
    "        datetime(2025, 9, 19, 11, 15),\n",
    "        [\"Java\", \"Scala\"],\n",
    "        {\"theme\": \"light\", \"lang\": \"fr\"},\n",
    "        {\"city\": \"Pune\", \"zip\": \"411001\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55b8057-0f16-4c69-b994-5e4d891c79d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddl_schema = \"\"\"\n",
    "    name STRING,\n",
    "    age INT,\n",
    "    user_id BIGINT,\n",
    "    price DOUBLE,\n",
    "    is_active BOOLEAN,\n",
    "    birthdate DATE,\n",
    "    event_time TIMESTAMP,\n",
    "    skills ARRAY<STRING>,\n",
    "    preferences MAP<STRING, STRING>,\n",
    "    address STRUCT<city: STRING, zip: STRING>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38749891-d2b2-4c9e-84cb-7e6919a86131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema=ddl_schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_read_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
